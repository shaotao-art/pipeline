{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/starfish/pattern_reg/data/english_to_french.txt\", mode='r', encoding='utf') as f:\n",
    "    lines = f.readlines(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [line.split(\"\\t\") for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # split\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    # keep only words\n",
    "    tokens = [token for token in tokens \n",
    "            if all(char.isalpha() for char in token)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, target = [tokenize(pairs[i][0]) for i in range(len(pairs))], [tokenize(pairs[i][1]) for i in range(len(pairs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_982/2310129780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'com string-dsad'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "a = 'com string-dsad'\n",
    "a.split().split(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "padding_token = '<PAD>'\n",
    "start_of_sequence_token = '<SOS>'\n",
    "end_of_sequence_token = '<EOS>'\n",
    "unknown_word_token = '<UNK>'\n",
    "vac_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vac(src, padding_token, start_of_sequence_token, end_of_sequence_token, unknown_word_token, vac_size=30):\n",
    "    flatten = lambda x: [sublst for lst in x for sublst in lst]\n",
    "    token_set = flatten(src)\n",
    "    commons = Counter(token_set).most_common(vac_size - 4)\n",
    "    token2idx = {commons[i][0]: (i + 4) for i in range(len(commons))}\n",
    "    token2idx.update({unknown_word_token:0, start_of_sequence_token:1, end_of_sequence_token:2, padding_token:3})\n",
    "    idx2token = {value: key for key, value in token2idx.items()}\n",
    "\n",
    "    return token2idx, idx2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_token2idx, src_idx2token = build_vac(src, padding_token, start_of_sequence_token, end_of_sequence_token, unknown_word_token)\n",
    "tar_token2idx, tar_idx2token = build_vac(target, padding_token, start_of_sequence_token, end_of_sequence_token, unknown_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rare_tokens(src, token2idx, unknown_word_token):\n",
    "    for i in range(len(src)):\n",
    "        for j in range(len(src[i])):\n",
    "            if src[i][j] not in token2idx:\n",
    "                src[i][j] = unknown_word_token\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = replace_rare_tokens(src, src_token2idx, unknown_word_token)\n",
    "target = replace_rare_tokens(target, tar_token2idx, unknown_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_most_unk(threshold, src, target):\n",
    "    idx = []\n",
    "    src_ratio = [sentence.count(\"<UNK>\") / len(sentence) for sentence in src]\n",
    "    tar_ration = [sentence.count(\"<UNK>\") / len(sentence) for sentence in target]\n",
    "    for i in range(len(src)):\n",
    "        if src_ratio[i] < threshold and tar_ration[i] < threshold:\n",
    "            idx.append(i)\n",
    "    src_out = [src[_] for _ in idx]\n",
    "    target_out = [target[_] for _ in idx]\n",
    "    return src_out, target_out\n",
    "src, target = remove_most_unk(0.3, src, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_and_end(src, start_of_sequence_token, end_of_sequence_token):\n",
    "    for i in range(len(src)):\n",
    "        src[i] = [start_of_sequence_token] + src[i] + [end_of_sequence_token]\n",
    "    return src \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = add_start_and_end(src, start_of_sequence_token, end_of_sequence_token)\n",
    "target = add_start_and_end(target, start_of_sequence_token, end_of_sequence_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['<SOS>', 'go', '<EOS>'],\n",
       "  ['<SOS>', 'run', '<EOS>'],\n",
       "  ['<SOS>', 'run', '<EOS>'],\n",
       "  ['<SOS>', 'wow', '<EOS>'],\n",
       "  ['<SOS>', 'fire', '<EOS>'],\n",
       "  ['<SOS>', 'i', 'won', '<EOS>'],\n",
       "  ['<SOS>', 'cheers', '<EOS>'],\n",
       "  ['<SOS>', 'cheers', '<EOS>'],\n",
       "  ['<SOS>', 'got', 'it', '<EOS>'],\n",
       "  ['<SOS>', 'got', 'it', '<EOS>'],\n",
       "  ['<SOS>', 'got', 'it', '<EOS>'],\n",
       "  ['<SOS>', 'got', 'it', '<EOS>'],\n",
       "  ['<SOS>', 'i', 'm', 'ok', '<EOS>'],\n",
       "  ['<SOS>', 'no', 'way', '<EOS>']],\n",
       " [['<SOS>', 'va', '<EOS>'],\n",
       "  ['<SOS>', 'cours', '<EOS>'],\n",
       "  ['<SOS>', 'courez', '<EOS>'],\n",
       "  ['<SOS>', 'ça', 'alors', '<EOS>'],\n",
       "  ['<SOS>', 'au', 'feu', '<EOS>'],\n",
       "  ['<SOS>', 'je', 'l', 'ai', '<UNK>', '<EOS>'],\n",
       "  ['<SOS>', 'santé', '<EOS>'],\n",
       "  ['<SOS>', 'tchin', 'tchin', '<EOS>'],\n",
       "  ['<SOS>', 'j', 'ai', 'pigé', '<EOS>'],\n",
       "  ['<SOS>', 'compris', '<EOS>'],\n",
       "  ['<SOS>', 'pigé', '<EOS>'],\n",
       "  ['<SOS>', 'compris', '<EOS>'],\n",
       "  ['<SOS>', 'ça', 'va', '<EOS>'],\n",
       "  ['<SOS>', 'c', 'est', 'pas', '<UNK>', '<EOS>']])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_idx(src, token2idx):\n",
    "    for i in range(len(src)):\n",
    "        for j in range(len(src[i])):\n",
    "            src[i][j] = token2idx[src[i][j]]\n",
    "    return src\n",
    "src = tokens_to_idx(src, src_token2idx)\n",
    "target = tokens_to_idx(target, tar_token2idx) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 23, 2],\n",
       " [1, 12, 2],\n",
       " [1, 12, 2],\n",
       " [1, 24, 2],\n",
       " [1, 25, 2],\n",
       " [1, 4, 14, 2],\n",
       " [1, 9, 2],\n",
       " [1, 9, 2],\n",
       " [1, 7, 8, 2],\n",
       " [1, 7, 8, 2],\n",
       " [1, 7, 8, 2],\n",
       " [1, 7, 8, 2],\n",
       " [1, 4, 11, 22, 2],\n",
       " [1, 5, 6, 2]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'as': '1', 'aas': '22'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def allocate_fn():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = [torch.LongTensor(item[0]) for item in batch]\n",
    "    targets = [torch.LongTensor(item[1]) for item in batch]\n",
    "    \n",
    "    # Pad sequencse so that they are all the same length (within one minibatch)\n",
    "    padded_inputs = pad_sequence(inputs, padding_value=dataset.padding_token_value, batch_first=True)\n",
    "    padded_targets = pad_sequence(targets, padding_value=dataset.padding_token_value, batch_first=True)\n",
    "    \n",
    "    # Sort by length for CUDA optimizations\n",
    "    lengths = torch.LongTensor([len(x) for x in inputs])\n",
    "    lengths, permutation = lengths.sort(dim=0, descending=True)\n",
    "\n",
    "    # lengths is the len of src\n",
    "    return padded_inputs[permutation], padded_targets[permutation], lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationData(Dataset):\n",
    "    def __init__(self, src, target, padding_token_value):\n",
    "        self.src = src\n",
    "        self.target = target\n",
    "        self.padding_token_value = padding_token_value\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.target[idx]\n",
    "\n",
    "dataset = TranslationData(src, target, 3)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  4, 11, 22,  2],\n",
       "         [ 1,  7,  8,  2,  3],\n",
       "         [ 1,  5,  6,  2,  3],\n",
       "         [ 1,  7,  8,  2,  3],\n",
       "         [ 1,  7,  8,  2,  3],\n",
       "         [ 1,  7,  8,  2,  3],\n",
       "         [ 1,  4, 14,  2,  3],\n",
       "         [ 1, 24,  2,  3,  3],\n",
       "         [ 1, 12,  2,  3,  3],\n",
       "         [ 1, 23,  2,  3,  3],\n",
       "         [ 1, 25,  2,  3,  3],\n",
       "         [ 1,  9,  2,  3,  3],\n",
       "         [ 1, 12,  2,  3,  3],\n",
       "         [ 1,  9,  2,  3,  3]]),\n",
       " tensor([[ 1,  9, 12,  2,  3,  3],\n",
       "         [ 1, 19,  2,  3,  3,  3],\n",
       "         [ 1, 10,  8, 23,  0,  2],\n",
       "         [ 1, 19,  2,  3,  3,  3],\n",
       "         [ 1,  5,  6, 18,  2,  3],\n",
       "         [ 1, 18,  2,  3,  3,  3],\n",
       "         [ 1,  4, 14,  6,  0,  2],\n",
       "         [ 1,  9, 27,  2,  3,  3],\n",
       "         [ 1, 25,  2,  3,  3,  3],\n",
       "         [ 1, 12,  2,  3,  3,  3],\n",
       "         [ 1, 28, 29,  2,  3,  3],\n",
       "         [ 1, 17, 17,  2,  3,  3],\n",
       "         [ 1, 26,  2,  3,  3,  3],\n",
       "         [ 1, 16,  2,  3,  3,  3]]),\n",
       " tensor([5, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.GRU(input_size=128, hidden_size=256, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_hidden = torch.randn(1, 8, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, state = model(x, init_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a4a2141afdff6df85941b7817127327f4bc12be9b9d0589cd269c353a6e39b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
